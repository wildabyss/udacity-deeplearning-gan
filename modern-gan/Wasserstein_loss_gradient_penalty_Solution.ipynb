{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wasserstein Loss and Gradient penalty\n",
    "\n",
    "In the course, we discussed the limitations of the binary cross entropy loss (BCE Loss). It can lead to:\n",
    "* [mode collapse](https://developers.google.com/machine-learning/gan/problems): when the generator is stuck in a single mode of the distribution it is trying to replicate. For example, a generator training on the MNIST dataset would be stuck in generating images of certain digits only, as shown below.\n",
    "\n",
    "<img src='assets/collapse.png' width=50% />\n",
    "\n",
    "* vanishing gradient: it's a common problem with many neural networks architectures but is very common when training GANs. Because the discriminator's task is much easier than the generator's, the discriminator tends to converge faster and reach a high accuracy. The discriminator loss gets close to zero and the gradients become very small, leading to that vanishing gradient problem. \n",
    "\n",
    "In this notebook, you will:\n",
    "* implement the Wasserstein Loss\n",
    "* implement two types of gradient penalties\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wasserstein Loss\n",
    "\n",
    "The [Wasserstein GAN paper](https://arxiv.org/pdf/1701.07875.pdf) introduced a new type of loss function: the [Wasserstein Distance](https://en.wikipedia.org/wiki/Wasserstein_metric). We are now reshaping the problem GANs are solving: instead of having a loss function that classifies a distribution as being real or not, we have a loss function that tries to minimize the distance between the real and the fake distribution. The difference is subtle but plays a big role in the stability of GANs\n",
    "\n",
    "<img src='assets/gradient_replace.png' width=80% />\n",
    "\n",
    "The discriminator is now called a **critic** because it's job is not really to distinguish between real and fake anymore but to maximize the distance between the two distributions. However, we will be using both terms interchangeably for the sake of clarity. \n",
    "\n",
    "The Wasserstein loss can be calculated using the formula below:\n",
    "\n",
    "<center>$\\min_{g} \\max_{c} E(c(x)) - E(c(g(z)))$</center>\n",
    "\n",
    "You are now familiar with the minimax function. The main difference with the BCE Loss is the disapperance of the logs!\n",
    "\n",
    "### First exercise: implement the Wasserstein Loss\n",
    "\n",
    "The Wasserstein Loss (W-Loss) is taking the vector of logits outputed by the discriminator as input. In comparison, the BCE Loss was taking the probabilities (logits after a softmax layer) as inputs. The discriminator W-Loss is trying to maximize the mean value of the logits of real images and minize the mean value of the logits of fake images. The generator W-Loss is trying to maximize the mean value of the logits of fake images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disc_w_loss(real_logits: torch.Tensor, fake_logits: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Wasserstein Discriminator Loss\n",
    "    \n",
    "    args:\n",
    "    - real_logits: vector of logits outputed by the discriminator with a real input image\n",
    "    - fake_logits: vector of logits outputed by the discriminator with a fake input image \n",
    "    \"\"\"\n",
    "    real_loss = -real_logits.mean()\n",
    "    fake_loss = fake_logits.mean()\n",
    "    return real_loss + fake_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disc_g_loss(fake_logits: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Wasserstein Generator Loss\n",
    "    \n",
    "    args:\n",
    "    - fake_logits: vector of logits outputed by the discriminator with a fake input image \n",
    "    \"\"\"\n",
    "    fake_loss = -fake_logits.mean()\n",
    "    return fake_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congratulations, you successfully implemented the W-Loss for the discriminator\n"
     ]
    }
   ],
   "source": [
    "tests.check_disc_w_loss(disc_w_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congratulations, you successfully implemented the W-Loss for the generator\n"
     ]
    }
   ],
   "source": [
    "tests.check_gen_w_loss(disc_g_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient penalty\n",
    "\n",
    "To train a GAN with the Wasserstein Loss, the discriminator (or critic) must be [1-Lipschitz continuous](https://en.wikipedia.org/wiki/Lipschitz_continuity). \n",
    "\n",
    "The 1-Lipschitz continuity constraint implies that the norm of the gradient of the function must be below 1. In other words, for a function $f(x)$:\n",
    "\n",
    "</br>\n",
    "<center>$|| \\frac{df}{dx} || < 1$</center>\n",
    "\n",
    "Because the W-Loss is not bounded between 0 and 1 like the BCE loss, the above constraint makes sure that the loss does not grow too much. \n",
    "\n",
    "In the original paper, the authors enforced this condition by using weight clipping. However, per their own words:\n",
    "\n",
    "```\n",
    "Weight clipping is a clearly terrible way to enforce a Lipschitz constraint. If the\n",
    "clipping parameter is large, then it can take a long time for any weights to reach\n",
    "their limit, thereby making it harder to train the critic till optimality. If the clipping\n",
    "is small, this can easily lead to vanishing gradients when the number of layers is\n",
    "big, or batch normalization is not used (such as in RNNs). We experimented with\n",
    "simple variants (such as projecting the weights to a sphere) with little difference, and\n",
    "we stuck with weight clipping due to its simplicity and already good performance.\n",
    "However, we do leave the topic of enforcing Lipschitz constraints in a neural network\n",
    "setting for further investigation, and we actively encourage interested researchers\n",
    "to improve on this method.\n",
    "```\n",
    "\n",
    "## WGAN-GP\n",
    "Introducing Wasserstein Gan with Gradient Penalty, or [WGAN-GP](https://arxiv.org/pdf/1704.00028.pdf). In this paper, the author introduce a more robust way to enforce the 1-Lipschitz constaint of the critic: a **gradient penalty term** in the loss function. The new loss function is described below:\n",
    "\n",
    "<img src='assets/wgan_gp.png' width=80% />\n",
    "\n",
    "\n",
    "The gradient penalty is calculated as follow:\n",
    "* sample a random point $\\hat{x}$ between the generated distribution and the real distribution. \n",
    "* run this sample through the discriminator and calculate the gradient $\\nabla_{\\hat{x}} D(\\hat{x})$\n",
    "* calculate the L2 norm of the gradient $|| \\nabla_{\\hat{x}} D(\\hat{x}) ||_{2}$\n",
    "* remove 1, square the result and calculate the mean $(|| \\nabla_{\\hat{x}} D(\\hat{x}) ||_{2} - 1) ^{2}$\n",
    "\n",
    "### Second exercise: implement the gradient penalty\n",
    "\n",
    "In the second exercise of this notebook, you will implement the above gradient penalty. To help you, I have created a dummy critic module.\n",
    "\n",
    "**Tips**:\n",
    "* to calculate the gradients, you first have to set the attribute of a tensor `requires_grad` to True.\n",
    "* you can use the following code to calculate the gradients:\n",
    "```\n",
    "torch.autograd.grad(critic(x), x, grad_outputs=torch.ones_like(critic(x)), create_graph=True)[0]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    \"\"\" \n",
    "    Dummy critic class \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.pow(x, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(real_sample: torch.Tensor, \n",
    "                     fake_sample: torch.Tensor,\n",
    "                     critic: nn.Module) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Gradient penalty of the WGAN-GP model\n",
    "    args:\n",
    "    - real_sample: sample from the real dataset\n",
    "    - fake_sample: generated sample\n",
    "    \n",
    "    returns:\n",
    "    - gradient penalty\n",
    "    \"\"\"\n",
    "    # sample a random point between both distributions\n",
    "    alpha = torch.rand(real_sample.shape)\n",
    "    x_hat = alpha * real_sample + (1 - alpha) * fake_sample\n",
    "    \n",
    "    # calculate the gradient\n",
    "    x_hat.requires_grad = True\n",
    "    pred = critic(x_hat)\n",
    "    grad = torch.autograd.grad(pred, \n",
    "                               x_hat, \n",
    "                               grad_outputs=torch.ones_like(pred), \n",
    "                               create_graph=True)[0]\n",
    "    \n",
    "    # calculate the norm and the final penalty\n",
    "    norm = torch.norm(grad.view(-1), 2)\n",
    "    gp = ((norm - 1)**2).mean()    \n",
    "    return gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_sample = torch.randn(3, 32, 32)\n",
    "fake_sample = torch.randn(3, 32, 32)\n",
    "critic = Critic()\n",
    "\n",
    "gradient_penalty = gradient_penalty(real_sample, fake_sample, critic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DRAGAN\n",
    "\n",
    "The [DRAGAN paper](https://arxiv.org/pdf/1705.07215.pdf) offered a different approach to calculate the gradient penalty and enforce the 1-Lipschitz constraint on the critic.\n",
    "\n",
    "<img src='assets/dragan_gp.png' width=60% />\n",
    "\n",
    "As you can see, the formula is very similar, especially since the authors use $k = 1$ for their experiments. The main difference with the WGAN-GP gradient penalty is the $\\delta$ term, which is a noise term. In their implementation, the authors calculate $X_{p} = X + \\delta $ as follow:\n",
    "\n",
    "<center>\n",
    "    $X_{p} = X + 0.5 * \\sigma({X}) * N$ \n",
    "</center>\n",
    "\n",
    "where $\\sigma$ is the standard deviation and $N$ a noise term sampled from the uniform distribution.\n",
    "\n",
    "The gradient penalty is then calculated as follow:\n",
    "* sample a random point $\\hat{x}$ between the real distribution $X$ and $X_{p}$ . \n",
    "* run this sample through the discriminator and calculate the gradient $\\nabla_{\\hat{x}} D(\\hat{x})$\n",
    "* calculate the L2 norm of the gradient $|| \\nabla_{\\hat{x}} D(\\hat{x}) ||_{2}$\n",
    "* remove 1, square the result and calculate the mean $(|| \\nabla_{\\hat{x}} D(\\hat{x}) ||_{2} - 1) ^{2}$\n",
    "\n",
    "\n",
    "### BCE Loss\n",
    "Interestingly, using this gradient penalty lifts some of the constraint on the BCE Loss and the author use the above gradient penalty with the vanilla GAN losses (BCE Loss).\n",
    "\n",
    "### Third exercise: implement the DRAGAN gradient penalty\n",
    "\n",
    "In the third exercise of this notebook, you will implement the DRAGAN gradient penalty. This is a one liner difference with the above implementation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty_dragan(real_sample: torch.Tensor, critic: nn.Module) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Gradient penalty of the WGAN-GP model\n",
    "    args:\n",
    "    - real_sample: sample from the real dataset\n",
    "    \n",
    "    returns:\n",
    "    - gradient penalty\n",
    "    \"\"\"\n",
    "    # sample a random point between both distributions\n",
    "    X_p = real_sample + 0.5 * real_sample.std() * torch.rand_like(real_sample)\n",
    "    \n",
    "    alpha = torch.rand(real_sample.shape)\n",
    "    x_hat = alpha * real_sample + (1 - alpha) * X_p\n",
    "    \n",
    "    # calculate the gradient\n",
    "    x_hat.requires_grad = True\n",
    "    pred = critic(x_hat)\n",
    "    grad = torch.autograd.grad(pred, \n",
    "                               x_hat, \n",
    "                               grad_outputs=torch.ones_like(pred), \n",
    "                               create_graph=True)[0]\n",
    "    \n",
    "    # calculate the norm and the final penalty\n",
    "    norm = torch.norm(grad.view(-1), 2)\n",
    "    gp = ((norm - 1)**2).mean()\n",
    "    return gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dragan_gp = gradient_penalty_dragan(real_sample, critic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WARNING\n",
    "\n",
    "The gradient penalty terms penalize each input to the critic individually. Therefore, the critic should a single input to a single output. However, we use some layers in the discriminator that remove this property: the BatchNormalization layers. The authors of the WGAN-GP paper explain the following:\n",
    "\n",
    "```\n",
    "No critic batch normalization Most prior GAN implementations use batch normalization in both the generator and the discriminator to help stabilize training, but batch normalization\n",
    "changes the form of the discriminator’s problem from mapping a single input to a single output to\n",
    "mapping from an entire batch of inputs to a batch of outputs . Our penalized training objective\n",
    "is no longer valid in this setting, since we penalize the norm of the critic’s gradient with respect\n",
    "to each input independently, and not the entire batch. To resolve this, we simply omit batch normalization in the critic in our models, finding that they perform well without it. Our method works\n",
    "with normalization schemes which don’t introduce correlations between examples. In particular, we\n",
    "recommend layer normalization as a drop-in replacement for batch normalization.\n",
    "```\n",
    "\n",
    "Keep this in mind if you decide to use the gradient penalty in your project! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
