{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progressive Growing of GANs\n",
    "\n",
    "By now, you probably have realized how difficult it can be to train GANs. They are fairly unstable, especially when trying to generated high dimensional samples, such as high resolution images! \n",
    "\n",
    "However, researchers never lack ideas to improve them and this 2017 paper made trainings of GANs more stable: [Progressive growing of GANs for improved quality, stability and variation](https://arxiv.org/pdf/1710.10196.pdf). \n",
    "\n",
    "The main idea behind this paper is the following: since training GANs on smaller images is easier, we can progressively grow the network and the generated images dimensions to make training easier for the network. It is illustrated by the figure below:\n",
    "\n",
    "<img src='assets/progan.png' width=80% />\n",
    "\n",
    "\n",
    "### Layer fading \n",
    "\n",
    "Each level, or depth, is training for a certain number of epochs (eg, 10 epochs). Then a new layer is added in the discriminator and the generator and we start training with these additional layers. However, when a new layer is added, it is fadded in smoothly, as described by the following figure:\n",
    "\n",
    "<img src='assets/layer_fading.png' width=70% />\n",
    "\n",
    "The `toRGB` and `fromRGB` layers are the layers projecting the feature vector to the RGB space (HxWx3) and the layer doing the opposite, respectively. \n",
    "\n",
    "Let's look at the example:\n",
    "* **(a)** the network is currrently training at 16x16 resolution, meaning that the generated images are 16x16x3\n",
    "* **(b)** we are adding two new layers to train at 32x32 resolution. However, we are fading the new layers by doing the following:\n",
    "    * for the generator, we take the output of the 16x16 layer and use nearest neighbor image resize to double its resolution to 32x32. The same output will also be fed to the 32x32 layer. Then we calculate the output of the network by doing a weighted sum of $(1- \\alpha)$ the upsampled 16x16 image and $\\alpha$ the 32x32 layer output. \n",
    "    * for the discriminator, we do something similar but to reduce the resolution, we use an average pooling layer\n",
    "    * the network trains for N epochs at each resolution. During the first $N/2$ epochs, we start with $/alpha = 0$ and increase alpha linearly to $/alpha = 1$. Then we train for the remaining $N/2$ epochs with $/alpha = 1$.\n",
    "* **(c)** the network is now training at 32x32 resolution\n",
    "\n",
    "#### Exercerise\n",
    "\n",
    "In this exercise, you will implement the Generator of the ProGan model. To make your life easier, I already implemented two torch modules: `GeneratorFirstBlock` and `GeneratorBlock`. \n",
    "* The `GeneratorFirstBlock` module takes the the latent vector as input and outputs a multi-dimensional feature maps\n",
    "* the `GeneratorBlock` module correspond to each layer added when increasing the resolution\n",
    "\n",
    "**Notes:** In the paper, the authors are using a new type of normalization, called PixelNormalization. I encourage you to read the paper but for the sake of simplicity, I did not add any normalization here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorFirstBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    This block follows the ProGan paper implementation.\n",
    "    Takes the latent vector and creates feature maps.\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim: int):\n",
    "        super(GeneratorFirstBlock, self).__init__()\n",
    "        # initial block \n",
    "        self.conv0 = nn.ConvTranspose2d(latent_dim, 512, kernel_size=4)\n",
    "        self.conv1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.activation = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # x is a (batch_size, latent_dim) latent vector, we need to turn it into a feature map\n",
    "        x = torch.unsqueeze(torch.unsqueeze(x, -1), -1)\n",
    "        x = self.conv0(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.activation(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    This block follows the ProGan paper implementation.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        super(GeneratorBlock, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
    "        self.activation = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = interpolate(x, scale_factor=2)\n",
    "        x = self.conv1(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.activation(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above two blocks, you can implement the Generator module. The end resolution that we want to reach is 512x512 and we will start at a 4x4 resolution. \n",
    "\n",
    "\n",
    "#### init\n",
    "The `__init__` method should contain enough blocks to work at full resolution. We are only instantiating the generator once! So you will need to:\n",
    "* create one GeneratorFirstBlock module\n",
    "* create enough GeneratorBlocks modules such that the final resolution is 512x512\n",
    "* create one `toRGB` layer per resolution. \n",
    "\n",
    "The number of filters in each layer is controlled by the `num_filters` function below.\n",
    "\n",
    "\n",
    "#### forward\n",
    "\n",
    "The forward method does the following:\n",
    "* takes the latent vector, the current resolution and `alpha` as input. \n",
    "* run the latent vector through the different blocks and perform `alpha` fading\n",
    "\n",
    "\n",
    "**Tips:**\n",
    "* you can the torch `interpolate` function to double the resolution of an image\n",
    "* you can use the `np.log2` function to map the resolution of the input image to a \"depth\" (or stage) level. For example, `np.log2(512) = 9` and `np.log2(4)` = 2.\n",
    "* when training at 4x4 resolution, you should not perform $\\alpha-$fading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tests\n",
    "\n",
    "from torch.nn.functional import interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_filters(stage: int, \n",
    "                fmap_base: int = 8192,\n",
    "                fmap_decay: float = 1.0,\n",
    "                fmap_max: int = 512): \n",
    "    \"\"\"\n",
    "    A small helper function to computer the number of filters for conv layers based on the depth.\n",
    "    From the original repo https://github.com/tkarras/progressive_growing_of_gans/blob/master/networks.py#L252\n",
    "    \"\"\"\n",
    "    return min(int(fmap_base / (2.0 ** (stage * fmap_decay))), fmap_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    Generator: takes a latent vector as input and output an image.\n",
    "    args:\n",
    "    - max_resolution: max image resolution\n",
    "    - latent_dim: dimension of the input latent vector\n",
    "    \"\"\"\n",
    "    def __init__(self, max_resolution: int, latent_dim: int):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # following the original implementation\n",
    "        resolution_log2 = int(np.log2(max_resolution))\n",
    "\n",
    "        # layers blocks\n",
    "        self.blocks = [GeneratorFirstBlock(latent_dim)]\n",
    "        for res in range(1, resolution_log2 - 1):\n",
    "            self.blocks.append(GeneratorBlock(num_filters(res), num_filters(res+1)))\n",
    "        self.blocks = nn.ModuleList(self.blocks)\n",
    "\n",
    "        # to rgb blocks\n",
    "        self.to_rgb = [nn.Conv2d(num_filters(res), 3, kernel_size=1) for res in range(1, resolution_log2)]\n",
    "        self.to_rgb = nn.ModuleList(self.to_rgb)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, current_res: int, alpha: float = 1.0):\n",
    "        resolution_log2 = int(np.log2(current_res))\n",
    "\n",
    "        # to rgb operation\n",
    "        if current_res == 4:\n",
    "            x = self.blocks[0](x)\n",
    "            images_out = self.to_rgb[0](x)\n",
    "        else:\n",
    "            # blocks\n",
    "            for block in self.blocks[:resolution_log2-2]:\n",
    "                x = block(x)\n",
    "\n",
    "            previous_img = self.to_rgb[resolution_log2-3](x)\n",
    "            previous_img_scaled = interpolate(previous_img, scale_factor=2)\n",
    "\n",
    "            x = self.blocks[resolution_log2-2](x)\n",
    "            new_img = self.to_rgb[resolution_log2-2](x)\n",
    "            images_out = new_img * alpha + (1 - alpha) * previous_img_scaled\n",
    "        return images_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(max_resolution=512, latent_dim=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests.check_progan_generator(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
